{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Jester Dataset Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sopolat/Gesture-Recognition-CNN-LSTM/blob/master/Copy_of_Jester_Dataset_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwjEqbZw--lo",
        "outputId": "e7ab04d9-7506-4da2-a5ed-787f94f0c514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYCZHcCHLzFa"
      },
      "source": [
        "# Load dependencies\n",
        "from tensorflow.keras.models import Sequential #The calling file will be changed in our application \n",
        "from tensorflow.keras.layers import Dense #The calling file will be changed in our application\n",
        "import numpy as np #The calling file will be changed in our application\n",
        "import tensorflow #The calling file will be changed in our application \n",
        "\n",
        "# Num rows\n",
        "num_rows = 5e8  #This will also be changed accordingly depending on our data set.\n",
        "batch_size = 250 #This will also be changed accordingly depending on our data set.\n",
        "\n",
        "\n",
        "# Load data\n",
        "def generate_arrays_from_file(path, batchsize):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    batchcount = 0\n",
        "    while True:\n",
        "        with open(path) as f:\n",
        "            for line in f:\n",
        "                x,y = line.split(',')\n",
        "                inputs.append(x)\n",
        "                targets.append(y)\n",
        "                batchcount += 1\n",
        "                if batchcount > batchsize:\n",
        "                  X = np.array(inputs, dtype='float32')\n",
        "                  y = np.array(targets, dtype='float32')\n",
        "                  yield (X, y)\n",
        "                  inputs = []\n",
        "                  targets = []\n",
        "                  batchsize = 0\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=1, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_absolute_error',\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              metrics=['mean_squared_error'])\n",
        "\n",
        "# Fit data to model\n",
        "model.fit(generate_arrays_from_file('./five_hundred.csv', batch_size),\n",
        "                    steps_per_epoch=num_rows / batch_size, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}